{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transforms import RandomAdjustBrightness, RandomAdjustContrast, RandomVerticalFlip, RandomHorizontalFlip, RandomAdjustColor\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import yaml\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from models import *\n",
    "from build_utils.datasets import *\n",
    "from build_utils.utils import *\n",
    "from train_utils import train_eval_utils as train_util\n",
    "from train_utils import get_coco_api_from_dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import util\n",
    "import transforms as T\n",
    "from coco_eval import CocoEvaluator\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import pickle\n",
    "opt = pickle.load(open('opt.pkl', 'rb'))\n",
    "hyp = pickle.load(open('hyp.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"weights\" + os.sep\n",
    "best = wdir + \"best.pt\"\n",
    "results_file = \"results{}.txt\".format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "cfg = opt.cfg\n",
    "data = opt.data\n",
    "epochs = opt.epochs\n",
    "batch_size = 1\n",
    "accumulate = max(round(64 / batch_size), 1)\n",
    "weights = opt.weights\n",
    "imgsz_train = opt.img_size\n",
    "imgsz_test = opt.img_size\n",
    "multi_scale = opt.multi_scale\n",
    "\n",
    "gs = 32\n",
    "assert math.fmod(imgsz_test, gs) == 0, \"--img-size %g must be a %g-multiple\" % (imgsz_test, gs)\n",
    "grid_min, grid_max = imgsz_test // gs, imgsz_test // gs\n",
    "if multi_scale:\n",
    "    imgsz_min = opt.img_size // 1.5\n",
    "    imgsz_max = opt.img_size // 0.667\n",
    "\n",
    "    grid_min, grid_max = imgsz_min // gs, imgsz_max // gs\n",
    "    imgsz_min, imgsz_max = int(grid_min * gs), int(grid_max * gs)\n",
    "    imgsz_train = imgsz_max\n",
    "\n",
    "data_dict = parse_data_cfg(data)\n",
    "train_path = data_dict[\"train\"]\n",
    "test_path = data_dict[\"valid\"]\n",
    "nc = 1 if opt.single_cls else int(data_dict[\"classes\"])\n",
    "hyp[\"cls\"] *= nc / 80\n",
    "hyp[\"obj\"] *= imgsz_test / 320\n",
    "\n",
    "for f in glob.glob(results_file):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet(cfg).to(device).eval()\n",
    "model.load_state_dict(torch.load(\"weights/best.pt\", map_location=device)['model'])\n",
    "model.nc = nc\n",
    "model.hyp = hyp\n",
    "model.gr = 1.0\n",
    "model.requires_grad_(False)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LoadImagesAndLabels(test_path, imgsz_test, batch_size,\n",
    "                                      hyp=hyp,\n",
    "                                      rect=True,\n",
    "                                      cache_images=opt.cache_images,\n",
    "                                      single_cls=opt.single_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datasetloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    num_workers=0,\n",
    "                                                    pin_memory=True,\n",
    "                                                    collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data/my_data_label.names\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = f.readlines()\n",
    "label_map = {}\n",
    "for i, label in enumerate(label_names):\n",
    "    label_map[i] = label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (imgs, targets, paths, _, _) in enumerate(val_datasetloader):\n",
    "    model.train()\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(imgs[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    imgs = imgs.to(device).float() / 255.0\n",
    "    imgs.requires_grad = True\n",
    "    targets = targets.to(device)\n",
    "    pred = model(imgs)\n",
    "    loss_dict = compute_loss(pred, targets, model)\n",
    "    loss = sum(loss for loss in loss_dict.values())\n",
    "    loss.backward()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def ifgsm_attack(image, target, epsilon, model, alpha, num_iter, momentum=0.3):\n",
    "    model = copy.deepcopy(model)\n",
    "    model.train()\n",
    "    rf_hori = RandomHorizontalFlip(0.5)\n",
    "    rf_vert = RandomVerticalFlip(0.5)\n",
    "    rf_hori_ = RandomHorizontalFlip(1)\n",
    "    rf_vert_ = RandomVerticalFlip(1)\n",
    "    rf_contrast = RandomAdjustContrast(0.4, 0.5)\n",
    "    rf_brightness = RandomAdjustBrightness(0.4, 0.5)\n",
    "    rf_color = RandomAdjustColor(0.3, 0.3)\n",
    "    image.requires_grad = True\n",
    "    last_grad = None\n",
    "    for i in range(num_iter):\n",
    "        image_hori, _, hori_trans = rf_hori(image, target)\n",
    "        image_hori = image_hori.detach()\n",
    "        image_hori.requires_grad = True\n",
    "        image_vert, _, vert_trans = rf_vert(image, target)\n",
    "        image_vert = image_vert.detach()\n",
    "        image_vert.requires_grad = True\n",
    "        image_contrast, _, contrast_trans = rf_contrast(image, target)\n",
    "        image_contrast = image_contrast.detach()\n",
    "        image_contrast.requires_grad = True\n",
    "        image_brightness, _, brightness_trans = rf_brightness(image, target)\n",
    "        image_brightness = image_brightness.detach()\n",
    "        image_brightness.requires_grad = True\n",
    "        image_color, _, color_trans = rf_color(image, target)\n",
    "        image_color = image_color.detach()\n",
    "        image_color.requires_grad = True\n",
    "        pred = model(image)\n",
    "        loss_dict = compute_loss(pred, target, model)\n",
    "        pred_hori = model(image_hori) if hori_trans else None\n",
    "        loss_hori_dict = compute_loss(pred_hori,target,model) if hori_trans else {\"0\": 0}\n",
    "        pred_vert = model(image_vert) if vert_trans else None\n",
    "        loss_vert_dict = compute_loss(pred_vert,target,model) if vert_trans else {\"0\": 0}\n",
    "        pred_contrast = model(image_contrast) if contrast_trans else None\n",
    "        loss_contrast_dict = compute_loss(pred_contrast,target,model) if contrast_trans else {\"0\": 0}\n",
    "        pred_brightness = model(image_brightness) if brightness_trans else None\n",
    "        loss_brightness_dict = compute_loss(pred_brightness,target,model) if brightness_trans else {\"0\": 0}\n",
    "        pred_color = model(image_color) if color_trans else None\n",
    "        loss_color_dict = compute_loss(pred_color,target,model) if color_trans else {\"0\": 0}\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_hori = sum(loss for loss in loss_hori_dict.values())\n",
    "        loss_vert = sum(loss for loss in loss_vert_dict.values())\n",
    "        loss_contrast = sum(loss for loss in loss_contrast_dict.values())\n",
    "        loss_brightness = sum(loss for loss in loss_brightness_dict.values())\n",
    "        loss_color = sum(loss for loss in loss_color_dict.values())\n",
    "        loss = loss + loss_hori + loss_vert + \\\n",
    "            loss_contrast + loss_brightness + loss_color\n",
    "        loss = loss / len([l for l in [loss, loss_hori, loss_vert,\n",
    "                          loss_contrast, loss_brightness, loss_color] if l])\n",
    "        loss.backward()\n",
    "        image_grad = image.grad.data\n",
    "        image_hori_grad = image_hori.grad.data if hori_trans else None\n",
    "        image_vert_grad = image_vert.grad.data if vert_trans else None\n",
    "        image_contrast_grad = image_contrast.grad.data if contrast_trans else None\n",
    "        image_brightness_grad = image_brightness.grad.data if brightness_trans else None\n",
    "        image_color_grad = image_color.grad.data if color_trans else None\n",
    "        if hori_trans:\n",
    "            image_hori_grad, _, hori_trans = rf_hori_(image_hori_grad, target)\n",
    "        if vert_trans:\n",
    "            image_vert_grad, _, vert_trans = rf_vert_(image_vert_grad, target)\n",
    "        grad_list = [image_grad]\n",
    "        if hori_trans:\n",
    "            grad_list.append(image_hori_grad)\n",
    "        if vert_trans:\n",
    "            grad_list.append(image_vert_grad)\n",
    "        if contrast_trans:\n",
    "            grad_list.append(image_contrast_grad)\n",
    "        if brightness_trans:\n",
    "            grad_list.append(image_brightness_grad)\n",
    "        if color_trans:\n",
    "            grad_list.append(image_color_grad)\n",
    "        image_grad = torch.cat(grad_list, dim=0).mean(dim=0)\n",
    "        if last_grad is not None:\n",
    "            image_grad = image_grad + momentum * last_grad\n",
    "            last_grad = image_grad\n",
    "        else:\n",
    "            last_grad = image_grad\n",
    "        perturbed_image = image + alpha * image_grad.sign()\n",
    "        eta = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)\n",
    "        image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        image.requires_grad = True\n",
    "    model.eval()\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(image, target, epsilon, model, alpha, num_iter, norm=\"linf\"):\n",
    "    model = copy.deepcopy(model)\n",
    "    model.train()\n",
    "    if norm == \"inf\":\n",
    "        random_noise = torch.FloatTensor(\n",
    "            *image.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    else:\n",
    "        random_noise = random_noise = (\n",
    "            torch.randn(*image.shape) * epsilon).to(device)\n",
    "    image = image + random_noise\n",
    "    image.requires_grad = True\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        pred = model(image)\n",
    "        loss_dict = compute_loss(pred, target, model)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss.backward()\n",
    "        image_grad = image.grad.data\n",
    "        if norm == \"inf\":\n",
    "            perturbed_image = image + alpha * image_grad.sign()\n",
    "            eta = torch.clamp(perturbed_image - image,\n",
    "                              min=-epsilon, max=epsilon)\n",
    "            image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        else:\n",
    "            perturbed_image = image + alpha * image_grad\n",
    "            eta = torch.clamp(perturbed_image - image,\n",
    "                              min=-epsilon, max=epsilon)\n",
    "            eta = eta / eta.view(eta.shape[0], -\n",
    "                                 1).norm(dim=1).view(-1, 1, 1, 1)\n",
    "            image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        image.requires_grad = True\n",
    "    model.eval()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "\n",
    "def test_2000_samples(model, data_loader_test, device, num_iter=1, epsilon=0.1, alpha=0.1, momentum=0.3):\n",
    "    model.eval()\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco = get_coco_api_from_dataset(data_loader_test.dataset)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    for idx, (imgs, targets, paths,shapes, img_index) in tqdm(enumerate(data_loader_test), total=2000):\n",
    "        imgs = imgs.to(device) / 255.0\n",
    "        targets = targets.to(device)\n",
    "        # imgs = ifgsm_attack(imgs, targets, epsilon, model, alpha, num_iter, momentum=momentum)\n",
    "        imgs = pgd_attack(imgs, targets, epsilon, model, alpha, num_iter)\n",
    "        pred = model(imgs)[0]\n",
    "        # 将attack后的图片保存下来\n",
    "        # for i in range(len(imgs)):\n",
    "        #     img = imgs[i].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "        #     img = (img*255).astype(np.uint8)\n",
    "        #     img = Image.fromarray(img)\n",
    "        #     file_name = os.path.basename(paths[0])\n",
    "        #     img.save(f\"./data/coco_adv/val2017/{file_name}\")\n",
    "        pred = non_max_suppression(pred, conf_thres=0.01, iou_thres=0.6, multi_label=True)\n",
    "\n",
    "        outputs = []\n",
    "        for index, p in enumerate(pred):\n",
    "            if p is None:\n",
    "                p = torch.empty((0, 6), device=cpu_device)\n",
    "                boxes = torch.empty((0, 4), device=cpu_device)\n",
    "            else:\n",
    "                boxes = p[:, :4]\n",
    "                boxes = scale_coords(imgs[index].shape[1:], boxes, shapes[index][0]).round()\n",
    "\n",
    "            info = {\"boxes\": boxes.to(cpu_device),\n",
    "                    \"labels\": p[:, 5].to(device=cpu_device, dtype=torch.int64),\n",
    "                    \"scores\": p[:, 4].to(cpu_device)}\n",
    "            outputs.append(info)\n",
    "\n",
    "        res = {img_id: output for img_id, output in zip(img_index, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "        if idx == 2000:\n",
    "            break\n",
    "        del imgs, targets, outputs, res\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2000_samples(model, val_datasetloader, device, num_iter=5,epsilon=0.1,alpha=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "import cv2\n",
    "model.eval()\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "for idx, (imgs, targets, paths,shapes, img_index) in tqdm(enumerate(val_datasetloader), total=10):\n",
    "    imgs = imgs.to(device) / 255.0\n",
    "    targets = targets.to(device)\n",
    "    imgs_adv = ifgsm_attack(imgs, targets, 0.1, model, 0.005, 5, momentum=0.3)\n",
    "    pred = model(imgs)[0]\n",
    "    pred_adv = model(imgs_adv)[0]\n",
    "    pred = non_max_suppression(pred, conf_thres=0.01, iou_thres=0.6, multi_label=True)\n",
    "    pred_adv = non_max_suppression(pred_adv, conf_thres=0.01, iou_thres=0.6, multi_label=True)\n",
    "    outputs = []\n",
    "    for index, p in enumerate(pred):\n",
    "        if p is None:\n",
    "            p = torch.empty((0, 6), device=cpu_device)\n",
    "            boxes = torch.empty((0, 4), device=cpu_device)\n",
    "        else:\n",
    "            boxes = p[:, :4]\n",
    "            boxes = scale_coords(imgs[index].shape[1:], boxes, shapes[index][0]).round()\n",
    "\n",
    "        info = {\"boxes\": boxes.to(cpu_device),\n",
    "                \"labels\": p[:, 5].to(device=cpu_device, dtype=torch.int64),\n",
    "                \"scores\": p[:, 4].to(cpu_device)}\n",
    "        outputs.append(info)\n",
    "    \n",
    "    outputs_adv = []\n",
    "    for index, p in enumerate(pred_adv):\n",
    "        if p is None:\n",
    "            p = torch.empty((0, 6), device=cpu_device)\n",
    "            boxes = torch.empty((0, 4), device=cpu_device)\n",
    "        else:\n",
    "            boxes = p[:, :4]\n",
    "            boxes = scale_coords(imgs_adv[index].shape[1:], boxes, shapes[index][0]).round()\n",
    "\n",
    "        info = {\"boxes\": boxes.to(cpu_device),\n",
    "                \"labels\": p[:, 5].to(device=cpu_device, dtype=torch.int64),\n",
    "                \"scores\": p[:, 4].to(cpu_device)}\n",
    "        outputs_adv.append(info)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    imgs = cv2.resize(imgs[0].cpu().detach().permute(1, 2, 0).numpy(), shapes[index][0][::-1])\n",
    "    imgs_adv = cv2.resize(imgs_adv[0].cpu().detach().permute(1, 2, 0).numpy(), shapes[index][0][::-1])\n",
    "    axes[0].imshow(imgs)\n",
    "    for item in outputs:\n",
    "        boxes = item['boxes']\n",
    "        labels = item['labels']\n",
    "        scores = item['scores']\n",
    "        keep_thresh = 0.5\n",
    "        keep = scores > keep_thresh\n",
    "        boxes = boxes[keep].cpu().detach().numpy()\n",
    "        labels = labels[keep].cpu().detach().numpy()\n",
    "        for box, label in zip(boxes, labels):\n",
    "            rect = patches.Rectangle(\n",
    "                (box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "            axes[0].text(box[0], box[1], label_map[label],\n",
    "                         fontsize=12, color='r')\n",
    "            axes[0].add_patch(rect)\n",
    "    axes[1].imshow(imgs_adv)\n",
    "    for item in outputs_adv:\n",
    "        boxes = item['boxes']\n",
    "        labels = item['labels']\n",
    "        scores = item['scores']\n",
    "        keep_thresh = 0.5\n",
    "        keep = scores > keep_thresh\n",
    "        boxes = boxes[keep].cpu().detach().numpy()\n",
    "        labels = labels[keep].cpu().detach().numpy()\n",
    "        for box, label in zip(boxes, labels):\n",
    "            rect = patches.Rectangle(\n",
    "                (box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "            axes[1].text(box[0], box[1], label_map[label],\n",
    "                         fontsize=12, color='r')\n",
    "            axes[1].add_patch(rect)\n",
    "    if idx == 10:\n",
    "        break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1050a06cbaeed8b46d187604389f32b45fa537b377a0b8f76b38e0c23b5abbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
