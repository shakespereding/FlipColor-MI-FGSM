{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from transforms import RandomAdjustBrightness,RandomAdjustContrast,RandomVerticalFlip,RandomHorizontalFlip,RandomAdjustColor\n",
    "from coco_eval import CocoEvaluator\n",
    "from coco_utils import get_coco_api_from_dataset,get_coco,get_coco_kp\n",
    "import transforms as T\n",
    "import utils\n",
    "from PIL import Image\n",
    "\n",
    "def get_dataset(name, image_set, transform, data_path):\n",
    "    paths = {\n",
    "        \"coco\": (data_path, get_coco, 91),\n",
    "        \"coco_kp\": (data_path, get_coco_kp, 2),\n",
    "    }\n",
    "    p, ds_fn, num_classes = paths[name]\n",
    "\n",
    "    ds = ds_fn(p, image_set=image_set, transforms=transform)\n",
    "    return ds, num_classes\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "# dataset, num_classes = get_dataset(\"coco\", \"train\", get_transform(),\"data/coco\")\n",
    "dataset_test, num_classes = get_dataset(\"coco\", \"val\", get_transform(), \"data/coco\")\n",
    "# train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dt = json.load(open(\"data/coco/annotations/instances_val2017.json\"))\n",
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {k[\"id\"]: k[\"name\"] for k in dt[\"categories\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#         dataset, batch_sampler=train_batch_sampler, num_workers=args.workers,\n",
    "#         collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "model.to(device)\n",
    "model.train()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifgsm_attack(image,target, epsilon, model, alpha, num_iter,momentum=0.3):\n",
    "    model.train()\n",
    "    if isinstance(target, list):\n",
    "        target = target[0]\n",
    "    rf_hori = RandomHorizontalFlip(0.5)\n",
    "    rf_vert = RandomVerticalFlip(0.5)\n",
    "    rf_hori_ = RandomHorizontalFlip(1)\n",
    "    rf_vert_ = RandomVerticalFlip(1)\n",
    "    rf_contrast = RandomAdjustContrast(0.4,0.5)\n",
    "    rf_brightness = RandomAdjustBrightness(0.4,0.5)\n",
    "    rf_color = RandomAdjustColor(0.3,0.3)\n",
    "    image.requires_grad = True\n",
    "    last_grad = None\n",
    "    for i in range(num_iter):\n",
    "        image_hori,_,hori_trans = rf_hori(image,target)\n",
    "        image_hori = image_hori.detach()\n",
    "        image_hori.requires_grad = True\n",
    "        image_vert,_,vert_trans = rf_vert(image,target)\n",
    "        image_vert = image_vert.detach()\n",
    "        image_vert.requires_grad = True\n",
    "        image_contrast,_,contrast_trans = rf_contrast(image,target)\n",
    "        image_contrast = image_contrast.detach()\n",
    "        image_contrast.requires_grad = True\n",
    "        image_brightness,_,brightness_trans = rf_brightness(image,target)\n",
    "        image_brightness = image_brightness.detach()\n",
    "        image_brightness.requires_grad = True\n",
    "        image_color,_,color_trans = rf_color(image,target)\n",
    "        image_color = image_color.detach()\n",
    "        image_color.requires_grad = True\n",
    "        loss_dict = model(image,[target])\n",
    "        loss_hori_dict = model(image_hori,[target]) if hori_trans else {\"0\":0}\n",
    "        loss_vert_dict = model(image_vert,[target]) if vert_trans else {\"0\":0}\n",
    "        loss_contrast_dict = model(image_contrast,[target]) if contrast_trans else {\"0\":0}\n",
    "        loss_brightness_dict = model(image_brightness,[target]) if brightness_trans else {\"0\":0}\n",
    "        loss_color_dict = model(image_color,[target]) if color_trans else {\"0\":0}\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_hori = sum(loss for loss in loss_hori_dict.values())\n",
    "        loss_vert = sum(loss for loss in loss_vert_dict.values())\n",
    "        loss_contrast = sum(loss for loss in loss_contrast_dict.values())\n",
    "        loss_brightness = sum(loss for loss in loss_brightness_dict.values())\n",
    "        loss_color = sum(loss for loss in loss_color_dict.values())\n",
    "        loss = loss + loss_hori + loss_vert + loss_contrast + loss_brightness + loss_color\n",
    "        loss = loss / len([l for l in [loss,loss_hori,loss_vert,loss_contrast,loss_brightness,loss_color] if l])\n",
    "        loss.backward()\n",
    "        image_grad = image.grad.data\n",
    "        image_hori_grad = image_hori.grad.data if hori_trans else None\n",
    "        image_vert_grad = image_vert.grad.data if vert_trans else None\n",
    "        image_contrast_grad = image_contrast.grad.data if contrast_trans else None\n",
    "        image_brightness_grad = image_brightness.grad.data if brightness_trans else None\n",
    "        image_color_grad = image_color.grad.data if color_trans else None\n",
    "        if hori_trans:\n",
    "            image_hori_grad,_,hori_trans = rf_hori_(image_hori_grad,target)\n",
    "        if vert_trans:\n",
    "            image_vert_grad,_,vert_trans = rf_vert_(image_vert_grad,target)\n",
    "        grad_list = [image_grad]\n",
    "        if hori_trans:\n",
    "            grad_list.append(image_hori_grad)\n",
    "        if vert_trans:\n",
    "            grad_list.append(image_vert_grad)\n",
    "        if contrast_trans:\n",
    "            grad_list.append(image_contrast_grad)\n",
    "        if brightness_trans:\n",
    "            grad_list.append(image_brightness_grad)\n",
    "        if color_trans:\n",
    "            grad_list.append(image_color_grad)\n",
    "        image_grad = torch.cat(grad_list,dim=0).mean(dim=0)\n",
    "        if last_grad is not None:\n",
    "            image_grad = image_grad + momentum * last_grad\n",
    "            last_grad = image_grad\n",
    "        else:\n",
    "            last_grad = image_grad\n",
    "        perturbed_image = image + alpha * image_grad.sign()\n",
    "        eta = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)\n",
    "        image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        image.requires_grad = True\n",
    "    model.eval()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(image,target, epsilon, model, alpha, num_iter,momentum=0.3,norm=\"linf\"):\n",
    "    model.train()\n",
    "    if isinstance(target, list):\n",
    "        target = target[0]\n",
    "    rf_hori = RandomHorizontalFlip(0.5)\n",
    "    rf_vert = RandomVerticalFlip(0.5)\n",
    "    rf_hori_ = RandomHorizontalFlip(1)\n",
    "    rf_vert_ = RandomVerticalFlip(1)\n",
    "    rf_contrast = RandomAdjustContrast(0.4,0.5)\n",
    "    rf_brightness = RandomAdjustBrightness(0.4,0.5)\n",
    "    rf_color = RandomAdjustColor(0.3,0.3)\n",
    "    if norm == \"inf\":\n",
    "        random_noise = torch.FloatTensor(*image.shape).uniform_(-epsilon, epsilon).to(device)\n",
    "    else:\n",
    "        random_noise = random_noise = (torch.randn(*image.shape) * epsilon).to(device)\n",
    "    image = image + random_noise\n",
    "    image.requires_grad = True\n",
    "    last_grad = None\n",
    "    for i in range(num_iter):\n",
    "        image_hori,_,hori_trans = rf_hori(image,target)\n",
    "        image_hori = image_hori.detach()\n",
    "        image_hori.requires_grad = True\n",
    "        image_vert,_,vert_trans = rf_vert(image,target)\n",
    "        image_vert = image_vert.detach()\n",
    "        image_vert.requires_grad = True\n",
    "        image_contrast,_,contrast_trans = rf_contrast(image,target)\n",
    "        image_contrast = image_contrast.detach()\n",
    "        image_contrast.requires_grad = True\n",
    "        image_brightness,_,brightness_trans = rf_brightness(image,target)\n",
    "        image_brightness = image_brightness.detach()\n",
    "        image_brightness.requires_grad = True\n",
    "        image_color,_,color_trans = rf_color(image,target)\n",
    "        image_color = image_color.detach()\n",
    "        image_color.requires_grad = True\n",
    "        loss_dict = model(image,[target])\n",
    "        loss_hori_dict = model(image_hori,[target]) if hori_trans else {\"0\":0}\n",
    "        loss_vert_dict = model(image_vert,[target]) if vert_trans else {\"0\":0}\n",
    "        loss_contrast_dict = model(image_contrast,[target]) if contrast_trans else {\"0\":0}\n",
    "        loss_brightness_dict = model(image_brightness,[target]) if brightness_trans else {\"0\":0}\n",
    "        loss_color_dict = model(image_color,[target]) if color_trans else {\"0\":0}\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss_hori = sum(loss for loss in loss_hori_dict.values())\n",
    "        loss_vert = sum(loss for loss in loss_vert_dict.values())\n",
    "        loss_contrast = sum(loss for loss in loss_contrast_dict.values())\n",
    "        loss_brightness = sum(loss for loss in loss_brightness_dict.values())\n",
    "        loss_color = sum(loss for loss in loss_color_dict.values())\n",
    "        loss = loss + loss_hori + loss_vert + loss_contrast + loss_brightness + loss_color\n",
    "        loss = loss / len([l for l in [loss,loss_hori,loss_vert,loss_contrast,loss_brightness,loss_color] if l])\n",
    "        loss.backward()\n",
    "        image_grad = image.grad.data\n",
    "        image_hori_grad = image_hori.grad.data if hori_trans else None\n",
    "        image_vert_grad = image_vert.grad.data if vert_trans else None\n",
    "        image_contrast_grad = image_contrast.grad.data if contrast_trans else None\n",
    "        image_brightness_grad = image_brightness.grad.data if brightness_trans else None\n",
    "        image_color_grad = image_color.grad.data if color_trans else None\n",
    "        if hori_trans:\n",
    "            image_hori_grad,_,hori_trans = rf_hori_(image_hori_grad,target)\n",
    "        if vert_trans:\n",
    "            image_vert_grad,_,vert_trans = rf_vert_(image_vert_grad,target)\n",
    "        grad_list = [image_grad]\n",
    "        if hori_trans:\n",
    "            grad_list.append(image_hori_grad)\n",
    "        if vert_trans:\n",
    "            grad_list.append(image_vert_grad)\n",
    "        if contrast_trans:\n",
    "            grad_list.append(image_contrast_grad)\n",
    "        if brightness_trans:\n",
    "            grad_list.append(image_brightness_grad)\n",
    "        if color_trans:\n",
    "            grad_list.append(image_color_grad)\n",
    "        image_grad = torch.cat(grad_list,dim=0).mean(dim=0)\n",
    "        if last_grad is not None:\n",
    "            image_grad = image_grad + momentum * last_grad\n",
    "            last_grad = image_grad\n",
    "        else:\n",
    "            last_grad = image_grad\n",
    "        if norm == \"inf\":\n",
    "            perturbed_image = image + alpha * image_grad.sign()\n",
    "            eta = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)\n",
    "            image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        else:\n",
    "            perturbed_image = image + alpha * image_grad\n",
    "            eta = torch.clamp(perturbed_image - image, min=-epsilon, max=epsilon)\n",
    "            eta = eta / eta.view(eta.shape[0],-1).norm(dim=1).view(-1,1,1,1)\n",
    "            image = torch.clamp(image + eta, min=0, max=1).detach_()\n",
    "        image.requires_grad = True\n",
    "    model.eval()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "def test_2000_samples(model, data_loader_test, device, num_iter=1,epsilon=0.1,alpha=0.1,momentum=0.3):\n",
    "    model.eval()\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco = get_coco_api_from_dataset(data_loader_test.dataset)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    for idx,(images, targets) in tqdm(enumerate(data_loader_test),total=10):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        images = ifgsm_attack(images[0].unsqueeze(0),targets,epsilon,model,alpha,num_iter,momentum=momentum)\n",
    "        images = list(image.to(device) for image in images)\n",
    "        # images = pgd_attack(images[0].unsqueeze(0),targets,epsilon,model,alpha,num_iter,momentum=momentum)\n",
    "        # images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        outputs = model(images,targets)\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "        # 将attack后的图片保存下来\n",
    "        for i in range(len(images)):\n",
    "            img = images[i].cpu().numpy().transpose(1,2,0)\n",
    "            img = (img*255).astype(np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "            img.save(f\"./data/coco_adv/val2017/{targets[i]['image_id']}.jpg\")\n",
    "        if idx == 2000:\n",
    "            break\n",
    "        del images,targets,outputs,res\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_2000_samples(model, data_loader_test, device, num_iter=5,epsilon=0.1,alpha=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "for idx,(images, targets) in enumerate(data_loader_test):\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    # images_adv = ifgsm_attack(images[0].unsqueeze(0),targets,0.001,model,0.001,5,True,True,True,True)\n",
    "    images_adv = ifgsm_attack(images[0].unsqueeze(0),targets,0.1,model,0.005,5,momentum=0.3)\n",
    "    images_adv = list(image.to(device) for image in images_adv)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    outputs = model(images,targets)\n",
    "    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "    outputs_adv = model(images_adv,targets)\n",
    "    outputs_adv = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs_adv]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    axes[0].imshow(images[0].cpu().detach().permute(1,2,0))\n",
    "    for item in outputs:\n",
    "        boxes = item['boxes']\n",
    "        labels = item['labels']\n",
    "        scores = item['scores']\n",
    "        keep_thresh = 0.7\n",
    "        keep = scores > keep_thresh\n",
    "        boxes = boxes[keep].cpu().detach().numpy()\n",
    "        labels = labels[keep].cpu().detach().numpy()\n",
    "        for box,label in zip(boxes,labels):\n",
    "            rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n",
    "            axes[0].text(box[0],box[1],label_map[label],fontsize=12,color='r')\n",
    "            axes[0].add_patch(rect)\n",
    "    axes[1].imshow(images_adv[0].cpu().detach().permute(1,2,0))\n",
    "    for item in outputs_adv:\n",
    "        boxes = item['boxes']\n",
    "        labels = item['labels']\n",
    "        scores = item['scores']\n",
    "        keep_thresh = 0.7\n",
    "        keep = scores > keep_thresh\n",
    "        boxes = boxes[keep].cpu().detach().numpy()\n",
    "        labels = labels[keep].cpu().detach().numpy()\n",
    "        for box,label in zip(boxes,labels):\n",
    "            rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=1,edgecolor='r',facecolor='none')\n",
    "            axes[1].text(box[0],box[1],label_map[label],fontsize=12,color='r')\n",
    "            axes[1].add_patch(rect)\n",
    "    plt.savefig(f'./{idx}.png',dpi=300)\n",
    "    del images,targets,outputs,images_adv,outputs_adv,boxes,labels,scores,keep,keep_thresh\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if idx == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1050a06cbaeed8b46d187604389f32b45fa537b377a0b8f76b38e0c23b5abbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
